{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Challenge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "\n",
    "data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+--------+--------+-------+------------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|Embarked|Survived|idx_Sex|idx_Embarked|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+\n",
      "|     3|  male|22.0|    1|    0|       S|       0|    0.0|         0.0|\n",
      "|     1|female|38.0|    1|    0|       C|       1|    1.0|         1.0|\n",
      "|     3|female|26.0|    0|    0|       S|       1|    1.0|         0.0|\n",
      "|     1|female|35.0|    1|    0|       S|       1|    1.0|         0.0|\n",
      "|     3|  male|35.0|    0|    0|       S|       0|    0.0|         0.0|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" Select the Columns to use \"\"\"\n",
    "selected_data = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked', 'Survived']]\n",
    "\n",
    "\n",
    "#\"\"\" Drop the na columns \"\"\"\n",
    "clean_data = selected_data.na.drop()\n",
    "\n",
    "\n",
    "# \"\"\" Encode the String Columns \"\"\"\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer1 = StringIndexer(inputCol=\"Sex\", outputCol=\"idx_Sex\")\n",
    "indexer2 = StringIndexer(inputCol=\"Embarked\", outputCol=\"idx_Embarked\")\n",
    "\n",
    "\n",
    "processed_data = indexer1.fit(clean_data).transform(clean_data)\n",
    "processed_data = indexer2.fit(processed_data).transform(processed_data)\n",
    "\n",
    "processed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- idx_Sex: double (nullable = false)\n",
      " |-- idx_Embarked: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-----+-------+------------+\n",
      "|Pclass| Age|SibSp|Parch|idx_Sex|idx_Embarked|\n",
      "+------+----+-----+-----+-------+------------+\n",
      "|     3|22.0|    1|    0|    0.0|         0.0|\n",
      "|     1|38.0|    1|    0|    1.0|         1.0|\n",
      "|     3|26.0|    0|    0|    1.0|         0.0|\n",
      "|     1|35.0|    1|    0|    1.0|         0.0|\n",
      "|     3|35.0|    0|    0|    0.0|         0.0|\n",
      "|     1|54.0|    0|    0|    0.0|         0.0|\n",
      "|     3| 2.0|    3|    1|    0.0|         0.0|\n",
      "|     3|27.0|    0|    2|    1.0|         0.0|\n",
      "|     2|14.0|    1|    0|    1.0|         1.0|\n",
      "|     3| 4.0|    1|    1|    1.0|         0.0|\n",
      "+------+----+-----+-----+-------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'idx_Sex', 'idx_Embarked']\n",
    "processed_data2 = processed_data[processed_features]\n",
    "processed_data2.show(10)\n",
    "# processed_data2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|Embarked|Survived|idx_Sex|idx_Embarked|            features|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "|     3|  male|22.0|    1|    0|       S|       0|    0.0|         0.0|[3.0,22.0,1.0,0.0...|\n",
      "|     1|female|38.0|    1|    0|       C|       1|    1.0|         1.0|[1.0,38.0,1.0,0.0...|\n",
      "|     3|female|26.0|    0|    0|       S|       1|    1.0|         0.0|[3.0,26.0,0.0,0.0...|\n",
      "|     1|female|35.0|    1|    0|       S|       1|    1.0|         0.0|[1.0,35.0,1.0,0.0...|\n",
      "|     3|  male|35.0|    0|    0|       S|       0|    0.0|         0.0|(6,[0,1],[3.0,35.0])|\n",
      "|     1|  male|54.0|    0|    0|       S|       0|    0.0|         0.0|(6,[0,1],[1.0,54.0])|\n",
      "|     3|  male| 2.0|    3|    1|       S|       0|    0.0|         0.0|[3.0,2.0,3.0,1.0,...|\n",
      "|     3|female|27.0|    0|    2|       S|       1|    1.0|         0.0|[3.0,27.0,0.0,2.0...|\n",
      "|     2|female|14.0|    1|    0|       C|       1|    1.0|         1.0|[2.0,14.0,1.0,0.0...|\n",
      "|     3|female| 4.0|    1|    1|       S|       1|    1.0|         0.0|[3.0,4.0,1.0,1.0,...|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Split it into features and target \"\"\"\n",
    "processed_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'idx_Sex', 'idx_Embarked']\n",
    "target_column   = ['Survived']\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "vectorizer = VectorAssembler(inputCols=processed_features, outputCol='features')\n",
    "processed_data3 = vectorizer.transform(processed_data)\n",
    "processed_data3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(featuresCol='features', labelCol='Survived')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|Embarked|Survived|idx_Sex|idx_Embarked|            features|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "|     1|female|23.0|    1|    0|       C|       1|    1.0|         1.0|[1.0,23.0,1.0,0.0...|\n",
      "|     1|female|23.0|    3|    2|       S|       1|    1.0|         0.0|[1.0,23.0,3.0,2.0...|\n",
      "|     1|female|26.0|    0|    0|       S|       1|    1.0|         0.0|[1.0,26.0,0.0,0.0...|\n",
      "|     1|female|29.0|    0|    0|       S|       1|    1.0|         0.0|[1.0,29.0,0.0,0.0...|\n",
      "|     1|female|30.0|    0|    0|       C|       1|    1.0|         1.0|[1.0,30.0,0.0,0.0...|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = processed_data3.randomSplit([0.8, 0.2], seed=0)\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_training = random_forest_clf.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|Embarked|Survived|idx_Sex|idx_Embarked|            features|       rawPrediction|         probability|prediction|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|     1|female|23.0|    1|    0|       C|       1|    1.0|         1.0|[1.0,23.0,1.0,0.0...|[2.23054998685728...|[0.11152749934286...|       1.0|\n",
      "|     1|female|23.0|    3|    2|       S|       1|    1.0|         0.0|[1.0,23.0,3.0,2.0...|[4.30772716857011...|[0.21538635842850...|       1.0|\n",
      "|     1|female|26.0|    0|    0|       S|       1|    1.0|         0.0|[1.0,26.0,0.0,0.0...|[2.86645392786526...|[0.14332269639326...|       1.0|\n",
      "|     1|female|29.0|    0|    0|       S|       1|    1.0|         0.0|[1.0,29.0,0.0,0.0...|[1.56299953279796...|[0.07814997663989...|       1.0|\n",
      "|     1|female|30.0|    0|    0|       C|       1|    1.0|         1.0|[1.0,30.0,0.0,0.0...|[1.53026739610560...|[0.07651336980528...|       1.0|\n",
      "+------+------+----+-----+-----+--------+--------+-------+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_test = random_forest_training.transform(df_test)\n",
    "random_forest_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7801541599713133"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = criterion.evaluate(random_forest_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\halle\\Documents\\Software-Engineering-Challenge\\spark_ML.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/halle/Documents/Software-Engineering-Challenge/spark_ML.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m files:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/halle/Documents/Software-Engineering-Challenge/spark_ML.ipynb#ch0000015?line=1'>2</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(random_forest_training, files)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "with open('model_pkl', 'wb') as files:\n",
    "    pickle.dump(random_forest_training, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc1ca5f776d965e027b4b79c8cbd78d993506bb6ee7e574d17612d343bb0f3cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
